{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 간단한 챗봇 만들기\n",
    "- 데이터를 기반으로 응답하는 간단한 챗봇입니다."
   ],
   "id": "a3848da7442c20d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:15:37.223445Z",
     "start_time": "2025-05-09T14:15:37.216441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from http.client import responses\n",
    "\n",
    "from pyexpat.errors import messages\n",
    "\n",
    "\n",
    "def load_api_keys(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and \"=\" in line:\n",
    "                key, value = line.split(\"=\", 1)\n",
    "                os.environ[key.strip()] = value.strip()\n",
    "\n",
    "filepath = '../.env'\n",
    "\n",
    "# API 키 로드 및 환경변수 설정\n",
    "load_api_keys(filepath)"
   ],
   "id": "dc0782f315c0a328",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-09T14:33:46.799581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_list = [\n",
    "    'MLP-KTLim/llama-3-Korean-Bllossom-8B', # 너무 오래걸림\n",
    "    'microsoft/Phi-4-mini-instruct',\n",
    "    'UNIVA-Bllossom/DeepSeek-llama3.1-Bllossom-8B',\n",
    "]\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_list[0],\n",
    "    cache_dir='../_download/models',\n",
    "    revision='main',\n",
    "    # device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_list[0],\n",
    "    cache_dir='../_download/models',\n",
    "    revision='main',\n",
    "    # device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")"
   ],
   "id": "7c1a319e701d176c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\repository\\Lab\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\workspace\\repository\\Lab\\_download\\models\\models--MLP-KTLim--llama-3-Korean-Bllossom-8B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:31:05.338944Z",
     "start_time": "2025-05-09T14:31:05.331029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    # device=0,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generator)"
   ],
   "id": "eefe53b3f10c6748",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:33:28.214518Z",
     "start_time": "2025-05-09T14:33:25.596989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "words = [\"호랑이\", \"나무\"]\n",
    "\n",
    "class Result(BaseModel):\n",
    "    score: float = Field(..., description=\"연관성 점수, -1.0~1.0\")\n",
    "    reason: str = Field(..., description=\"점수에 대한 이유\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Result)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"너는 단어들의 연관성을 판단하는 언어학자야.\n",
    "    다음 주어지는 단어 두 개의 연관성을 판단하여 점수와 해당 점수를 매긴 이유를 출력해야해.\n",
    "\n",
    "    다음은 매겨야 할 점수의 상세 설명이야.\n",
    "    1. 점수의 범위: -1.000 ~ 1.000\n",
    "    2. 연관성에 따른 점수 -1에 가까울 수록 정반대의 연관성을 가지고 있으며 연관성이 없을 경우 0, 연관성이 많을경우 1에 가까운 점수를 나타냄.\n",
    "\n",
    "    응답 예시는 다음과 같아.\n",
    "    {{\n",
    "        score: 0.002,\n",
    "        season: 단어1과 단어2는 연관성이 거의 없다고 판단됩니다.\n",
    "    }}\n",
    "    답변은 무조건 한국어로 답해야 하며 반드시 응답포맷을 지킬 것.\n",
    "    \"\"\"),\n",
    "    (\"human\", f\"단어1: {words[0]}, 단어2: {words[1]}\"),\n",
    "    (\"system\", \"{format_instructions}\")\n",
    "])\n",
    "\n",
    "messages = prompt.format_messages(format_instructions=parser.get_format_instructions())\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "import re\n",
    "\n",
    "match = re.findall(r'\\{\\s*\"score\"\\s*:\\s*-?\\d+\\.\\d+,\\s*\"reason\"\\s*:\\s*\".*?\"\\s*\\}', response, re.DOTALL)\n",
    "result: BaseModel = None\n",
    "\n",
    "if match:\n",
    "    json_str = match[-1]\n",
    "    try:\n",
    "        result = parser.parse(json_str)\n",
    "    except Exception as e:\n",
    "        print(\"파싱 실패:\", e)\n",
    "else:\n",
    "    print(\"JSON 블록을 찾을 수 없습니다.\")\n",
    "    display(response)\n",
    "\n"
   ],
   "id": "b9852889736b040d",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:33:29.324244Z",
     "start_time": "2025-05-09T14:33:29.315439Z"
    }
   },
   "cell_type": "code",
   "source": "result",
   "id": "bd9965be42cecb65",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(score=-0.7, reason='단어1과 단어2는 서로 다른 주제에 속하며, 호랑이는 동물이고 나무는 식물로, 이 두 가지는 직접적인 연관성이 없습니다.')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
